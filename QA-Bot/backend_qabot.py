# -*- coding: utf-8 -*-
"""Backend_QABot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17kkLnQye6axLIAJtbariSROxVipraFS8

# Libraries
"""

from sentence_transformers import SentenceTransformer
from pinecone import Pinecone
import pdfplumber
from tabula import read_pdf
import pandas as pd
import streamlit as st
import numpy as np

"""# Initialize Connection"""

# Initialize Pinecone and the embedding model
def connect_pine():
  pc = Pinecone(api_key="pcsk_c63YW_RUaWvP7KH56wdEVPDQucvah8poY5DDq25ZTKBmmkdfSjVxF33tqVHpP6Y4w5HGV", environment="us-east-1")
  index = pc.Index("sample-set-jb")
  print("jb-CONNECTED")
  return index

"""# Process PDF"""

def process_pdf(file, index):
    """Process the uploaded PDF to extract P&L data and store embeddings."""
    filename = file.name.split(".")[0]  # Extract filename without extension

    tab_data = read_pdf(file, pages = 3)
    tab_df1 = pd.DataFrame(tab_data[0])

    # extract tables whose heading consists of Profit and Loss
    headings = []
    with pdfplumber.open(file) as pdf:
      for page_num, page in enumerate(pdf.pages):
        text = page.extract_text()
        lines = text.split("\n")

        for line in lines:
          if line.isupper():
            headings.append((page_num + 1, line.strip()))

    # Display the extracted headings
    for page_num, heading in headings:
      if "profit and loss" in heading.lower():  # Case-sensitive check
        print(f"==>> Page {page_num}: {heading}")
        ext_tab_data = read_pdf(file, pages = page_num, multiple_tables=True, stream=True)
        tab_df2 = pd.DataFrame(ext_tab_data[0])

    merged_table_df = pd.concat([tab_df1, tab_df2], ignore_index=True)
    merged_table_df['Condensed Consolidated Statement of Profit and Loss for the'] = merged_table_df['Condensed Consolidated Statement of Profit and Loss for the'].combine_first(merged_table_df['Particulars'])

    #correcting the structure to not miss any extracted information. [sub headings]
    occurrence_count = 0
    for idx, row in merged_table_df.iterrows():
        if row['Three months ended March 31,'] == '2024 2023':
            occurrence_count += 1
            if occurrence_count == 1:
                merged_table_df.at[idx, 'Condensed Consolidated Statement of Profit and Loss for the'] = 'Condensed Consolidated Statement of Profit and Loss for the'
            elif occurrence_count == 2:
                merged_table_df.at[idx, 'Condensed Consolidated Statement of Profit and Loss for the'] = 'Particulars'

    # renaming column name
    merged_table_df.rename(columns={'Condensed Consolidated Statement of Profit and Loss for the': 'Description'}, inplace=True)

    # null column removal
    merged_table_df = merged_table_df.loc[:, ~(merged_table_df.isna() | (merged_table_df == ' ')).all()]

    merged_table_df[['Three months ended March 31(2024)', 'Three months ended March 31(2023)']] = merged_table_df['Three months ended March 31,'].str.split(' ', expand=True)
    merged_table_df[['Year ended March 31(2024)', 'Year ended March 31(2023)']] = merged_table_df['Year ended March 31,'].str.split(' ', expand=True)
    merged_table_df.drop(columns=['Particulars', 'Three months ended March 31,','Year ended March 31,'], inplace=True)

    merged_table_df = merged_table_df.replace({None: 0.0})
    merged_table_df = merged_table_df.replace({np.nan: 0.0})

    data_for_embedding = merged_table_df.apply(lambda row: f"{row['Description']} | Note No: {row['Note No.']} | 2024 Q1: {row['Three months ended March 31(2024)']} | 2023 Q1: {row['Three months ended March 31(2023)']} | 2024 Year: {row['Year ended March 31(2024)']} | 2023 Year: {row['Year ended March 31(2023)']}", axis=1)

    model = SentenceTransformer('all-MiniLM-L6-v2')
    embeddings = model.encode(data_for_embedding.tolist()) # embedding of each row

    pinecone_data = [
        (
            int(i),  # Unique ID for each row
            embeddings[i].tolist(),  # Embedding vector
            # row.to_dict()  # Metadata from the original DataFrame
            merged_table_df.iloc[i].to_dict()
        )
        for i, (_, row) in enumerate(merged_table_df.iterrows())
    ]

    records = []
    for d, e in zip(pinecone_data, embeddings):
      records.append({
          "id": str(d[0]),
          "values": d[1],
          "metadata": d[2]
      })

    # Upsert the records into the index
    index.upsert(
        vectors=records,
        namespace=filename
    )
    print("jb - Data successfully upserted into Pinecone!")
    return page_num

"""# Queries"""

def test_queries(file, query, index):
  filename = file.name.split(".")[0]
  model = SentenceTransformer('all-MiniLM-L6-v2')
  query_embedding = model.encode([query])  # Encode the query into an embedding
  query_vector = query_embedding[0].tolist()
  # Query Pinecone for the top 3 most similar vectors
  print(index)
  results = index.query(
      vector=query_vector,
      top_k=3,              # Retrieve top 3 matches
      include_metadata=True,
      namespace=filename
  )
  print("jb-query retrieval")
  for match in results["matches"]:
    print(f"ID: {match['id']}, Score: {match['score']}, Metadata: {match['metadata']}")
  return results